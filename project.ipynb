{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Processing chunk 3\n",
      "Processing chunk 4\n",
      "Processing chunk 5\n",
      "Processing chunk 6\n",
      "Processing chunk 7\n",
      "Processing chunk 8\n",
      "Processing chunk 9\n",
      "Processing chunk 10\n",
      "Processing chunk 11\n",
      "Processing chunk 12\n",
      "Processing chunk 13\n",
      "Processing chunk 14\n",
      "Processing chunk 15\n",
      "Processing chunk 16\n",
      "Processing chunk 17\n",
      "Processing chunk 18\n",
      "Processing chunk 19\n",
      "Processing chunk 20\n",
      "Processing chunk 21\n",
      "Processing chunk 22\n",
      "Processing chunk 23\n",
      "Processing chunk 24\n",
      "Processing chunk 25\n",
      "Processing chunk 26\n",
      "Processing chunk 27\n",
      "Processing chunk 28\n",
      "Processing chunk 29\n",
      "Processing chunk 30\n",
      "Processing chunk 31\n",
      "Processing chunk 32\n",
      "Processing chunk 33\n",
      "Processing chunk 34\n",
      "Processing chunk 35\n",
      "Processing chunk 36\n",
      "Processing chunk 37\n",
      "Processing chunk 38\n",
      "Processing chunk 39\n",
      "Processing chunk 40\n",
      "Processing chunk 41\n",
      "Processing chunk 42\n",
      "Processing chunk 43\n",
      "Processing chunk 44\n",
      "Processing chunk 45\n",
      "Processing chunk 46\n",
      "Processing chunk 47\n",
      "Processing chunk 48\n",
      "Processing chunk 49\n",
      "Processing chunk 50\n",
      "Processing chunk 51\n",
      "Processing chunk 52\n",
      "Processing chunk 53\n",
      "Processing chunk 54\n",
      "Processing chunk 55\n",
      "Processing chunk 56\n",
      "Processing chunk 57\n",
      "Processing chunk 58\n",
      "Processing chunk 59\n",
      "Processing chunk 60\n",
      "Processing chunk 61\n",
      "Processing chunk 62\n",
      "Processing chunk 63\n",
      "Processing chunk 64\n",
      "Processing chunk 65\n",
      "Processing chunk 66\n",
      "Processing chunk 67\n",
      "Processing chunk 68\n",
      "Processing chunk 69\n",
      "Processing chunk 70\n",
      "Processing chunk 71\n",
      "Processing chunk 72\n",
      "Processing chunk 73\n",
      "Processing chunk 74\n",
      "Processing chunk 75\n",
      "Processing chunk 76\n",
      "Processing chunk 77\n",
      "Processing chunk 78\n",
      "Processing chunk 79\n",
      "Processing chunk 80\n",
      "Processing chunk 81\n",
      "Processing chunk 82\n",
      "Processing chunk 83\n",
      "Processing chunk 84\n",
      "Processing chunk 85\n",
      "Processing chunk 86\n",
      "Processing chunk 87\n",
      "Processing chunk 88\n",
      "Processing chunk 89\n",
      "Processing chunk 90\n",
      "Processing chunk 91\n",
      "Processing chunk 92\n",
      "Processing chunk 93\n",
      "Processing chunk 94\n",
      "Processing chunk 95\n",
      "Processing chunk 96\n",
      "Processing chunk 97\n",
      "Processing chunk 98\n",
      "Processing chunk 99\n",
      "Processing chunk 100\n",
      "Processing chunk 101\n",
      "Processing chunk 102\n",
      "Processing chunk 103\n",
      "Processing chunk 104\n",
      "Processing chunk 105\n",
      "Processing chunk 106\n",
      "Processing chunk 107\n",
      "Processing chunk 108\n",
      "Processing chunk 109\n",
      "Processing chunk 110\n",
      "Processing chunk 111\n",
      "Processing chunk 112\n",
      "Processing chunk 113\n",
      "Processing chunk 114\n",
      "Processing chunk 115\n",
      "Processing chunk 116\n",
      "Processing chunk 117\n",
      "Processing chunk 118\n",
      "Processing chunk 119\n",
      "Processing chunk 120\n",
      "Processing chunk 121\n",
      "Processing chunk 122\n",
      "Processing chunk 123\n",
      "Processing chunk 124\n",
      "Processing chunk 125\n",
      "Processing chunk 126\n",
      "Processing chunk 127\n",
      "Processing chunk 128\n",
      "Processing chunk 129\n",
      "Processing chunk 130\n",
      "Processing chunk 131\n",
      "Processing chunk 132\n",
      "Processing chunk 133\n",
      "Processing chunk 134\n",
      "Processing chunk 135\n",
      "Processing chunk 136\n",
      "Processing chunk 137\n",
      "Processing chunk 138\n"
     ]
    }
   ],
   "source": [
    "# Define the desired chunk size (25000 rows)\n",
    "chunk_size = 25000\n",
    "\n",
    "# Initialize a variable to store the last chunk\n",
    "last_chunk = None\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunks = pd.read_csv('Eartquakes-1990-2023.csv', chunksize=chunk_size)\n",
    "\n",
    "# Process each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Do whatever processing you need with each chunk\n",
    "    print(f\"Processing chunk {i+1}\")\n",
    "    # Update the last chunk\n",
    "    last_chunk = chunk\n",
    "\n",
    "# Save the last chunk to a separate CSV file\n",
    "last_chunk.to_csv('last_chunk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to new_file_last_25000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_21468\\1918219662.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset[['year', 'month', 'day', 'hour']] = df_subset['date'].apply(lambda x: pd.Series(extract_date_components(x)))\n",
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_21468\\1918219662.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset[['year', 'month', 'day', 'hour']] = df_subset['date'].apply(lambda x: pd.Series(extract_date_components(x)))\n",
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_21468\\1918219662.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset[['year', 'month', 'day', 'hour']] = df_subset['date'].apply(lambda x: pd.Series(extract_date_components(x)))\n",
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_21468\\1918219662.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset[['year', 'month', 'day', 'hour']] = df_subset['date'].apply(lambda x: pd.Series(extract_date_components(x)))\n",
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_21468\\1918219662.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset.drop(columns=['date'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Read data from file\n",
    "df = pd.read_csv('last_chunk.csv')\n",
    "\n",
    "# Drop rows with null values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Select the last 25000 rows\n",
    "num_rows_to_select = min(25000, len(df_cleaned))\n",
    "df_last_25000 = df_cleaned.tail(num_rows_to_select)\n",
    "\n",
    "# Function to extract year, month, day, and hour\n",
    "def extract_date_components(date_str):\n",
    "    date_time_obj = pd.to_datetime(date_str)\n",
    "    return date_time_obj.year, date_time_obj.month, date_time_obj.day, date_time_obj.hour\n",
    "\n",
    "# Extracting specific columns\n",
    "df_subset = df_last_25000[['longitude', 'latitude', 'significance', 'magnitudo', 'date']]\n",
    "\n",
    "# Extracting year, month, day, and hour\n",
    "df_subset[['year', 'month', 'day', 'hour']] = df_subset['date'].apply(lambda x: pd.Series(extract_date_components(x)))\n",
    "\n",
    "# Dropping the original 'date' column\n",
    "df_subset.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Save df_subset to a new CSV file\n",
    "df_subset.to_csv('new_file_last_25000.csv', index=False)\n",
    "\n",
    "print(\"Data saved to new_file_last_25000.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-122.7786667   38.8116667    2.        ...    6.           3.\n",
      "    14.       ]\n",
      " [-122.7778333   38.806        3.        ...    6.           3.\n",
      "    14.       ]\n",
      " [-122.7786636   38.8043327    1.        ...    6.           3.\n",
      "    14.       ]\n",
      " ...\n",
      " [-115.2968333   32.2331667   90.        ...    7.          29.\n",
      "    10.       ]\n",
      " [-122.800499    38.8274994   16.        ...    7.          29.\n",
      "    10.       ]\n",
      " [-155.051       57.5648      12.        ...    7.          29.\n",
      "    11.       ]]\n",
      "The shape(dimensions of the data: (20751, 8))\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('new_file_last_25000.csv', delimiter=',',skiprows=1)\n",
    "\n",
    "# # Drop the first row\n",
    "# data_array = data_array[1:]\n",
    "\n",
    "print(data)\n",
    "print(f\"The shape(dimensions of the data: {data.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (20751, 6)\n",
      "y shape:  (20751, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extracting features (longitude, latitude, year, month, day, hour)\n",
    "X = data[:, [0, 1, 4, 5, 6, 7]]\n",
    "\n",
    "# Extracting target variable (magnitudo)\n",
    "y = data[:, [3]]\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (12450, 6)\n",
      "y train shape:  (12450, 1)\n",
      "X CV shape: (4150, 6)\n",
      "X CV shape: (4150, 1)\n",
      "X test shape: (4151, 6)\n",
      "X test shape: (4151, 1)\n"
     ]
    }
   ],
   "source": [
    "## STEP 2\n",
    "\n",
    "x_train, x_, y_train, y_ = train_test_split(X, y, test_size=0.40, random_state=1)\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "del x_, y_\n",
    "\n",
    "print(\"X train shape: \",x_train.shape)\n",
    "print(\"y train shape: \",y_train.shape)\n",
    "\n",
    "print(\"X CV shape:\",x_cv.shape)\n",
    "print(\"X CV shape:\",y_cv.shape)\n",
    "\n",
    "print(\"X test shape:\",x_test.shape)\n",
    "print(\"X test shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 \n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Polynomial feature\n",
    "poly = PolynomialFeatures(degree = 3, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "X_test_mapped = poly.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using the z-score\n",
    "scaler = StandardScaler()\n",
    "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
    "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  0.32372428047031626\n",
      "Cross-Validation error:  0.32331802437831136\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_mapped_scaled,y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "yhat = model.predict(X_train_mapped_scaled)\n",
    "print(\"Training error: \", mean_squared_error(y_train,yhat) / 2)\n",
    "\n",
    "yhat = model.predict(X_cv_mapped_scaled)\n",
    "print(\"Cross-Validation error: \", mean_squared_error(y_cv,yhat) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/C2_W3_poly.png' width=50%>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mses = []\n",
    "cv_mses = []\n",
    "models = []\n",
    "polys = []\n",
    "scalers = []\n",
    "\n",
    "for degree in range(1,7):\n",
    "    \n",
    "    # Add polynomial features to the training set\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_train_mapped = poly.fit_transform(x_train)\n",
    "    polys.append(poly)\n",
    "    \n",
    "    # Scale the training set\n",
    "    scaler_poly = StandardScaler()\n",
    "    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "    scalers.append(scaler_poly)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_mapped_scaled, y_train )\n",
    "    models.append(model)\n",
    "    \n",
    "    # Compute the training MSE\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    train_mses.append(train_mse)\n",
    "    \n",
    "    # Add polynomial features and scale the cross validation set\n",
    "    X_cv_mapped = poly.transform(x_cv)\n",
    "    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "    \n",
    "    # Compute the cross validation MSE\n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    cv_mses.append(cv_mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest CV MSE is found in the model with degree= 4\n"
     ]
    }
   ],
   "source": [
    "degree = np.argmin(cv_mses) + 1\n",
    "print(f\"Lowest CV MSE is found in the model with degree= {degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.31\n",
      "Cross Validation MSE: 0.30\n",
      "Test MSE: 0.31\n"
     ]
    }
   ],
   "source": [
    "# Add polynomial features to the test set\n",
    "X_test_mapped = polys[degree-1].transform(x_test)\n",
    "\n",
    "# Scale the test set\n",
    "X_test_mapped_scaled = scalers[degree-1].transform(X_test_mapped)\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = models[degree-1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Training MSE: {train_mses[degree-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {cv_mses[degree-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "The same model selection process can also be used when choosing between different neural network architectures. In this section, you will create the models shown below and apply it to the same regression task above.\n",
    "\n",
    "<img src='images\\C2_W3_NN_Arch.png' width=40%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features\n",
    "degree = 1\n",
    "poly = PolynomialFeatures(degree, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "X_test_mapped = poly.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using the z-score\n",
    "scaler = StandardScaler()\n",
    "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
    "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models():\n",
    "    \n",
    "    tf.random.set_seed(20)\n",
    "    \n",
    "    model_1 = Sequential(\n",
    "        [\n",
    "            Dense(25, activation = 'relu'),\n",
    "            Dense(15, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_1'\n",
    "    )\n",
    "\n",
    "    model_2 = Sequential(\n",
    "        [\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_2'\n",
    "    )\n",
    "\n",
    "    model_3 = Sequential(\n",
    "        [\n",
    "            Dense(32, activation = 'relu'),\n",
    "            Dense(16, activation = 'relu'),\n",
    "            Dense(8, activation = 'relu'),\n",
    "            Dense(4, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_3'\n",
    "    )\n",
    "    \n",
    "    model_list = [model_1, model_2, model_3]\n",
    "    \n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model_1...\n",
      "Done!\n",
      "\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step\n",
      "Training model_2...\n",
      "Done!\n",
      "\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step\n",
      "Training model_3...\n",
      "Done!\n",
      "\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step\n",
      "RESULTS:\n",
      "Model 1: Training MSE: 0.37, CV MSE: 0.37\n",
      "Model 2: Training MSE: 0.74, CV MSE: 0.74\n",
      "Model 3: Training MSE: 0.74, CV MSE: 0.74\n"
     ]
    }
   ],
   "source": [
    "nn_train_mses = []\n",
    "nn_cv_mses = []\n",
    "\n",
    "nn_models = build_models()\n",
    "\n",
    "for model in nn_models:\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_mapped_scaled, y_train,\n",
    "        epochs=500,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "\n",
    "    # Record the training MSEs\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    nn_train_mses.append(train_mse)\n",
    "    \n",
    "    # Record the cross validation MSEs \n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    nn_cv_mses.append(cv_mse)\n",
    "\n",
    "    \n",
    "# print results\n",
    "print(\"RESULTS:\")\n",
    "for model_num in range(len(nn_train_mses)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n",
    "        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Selected Model: 1\n",
      "Training MSE: 0.37\n",
      "Cross Validation MSE: 0.37\n",
      "Test MSE: 0.38\n"
     ]
    }
   ],
   "source": [
    "# Select the model with the lowest CV MSE\n",
    "model_num = 1\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = nn_models[model_num-1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
