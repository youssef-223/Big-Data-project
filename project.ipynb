{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Import SkLearn ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_14544\\593018622.py:1: DtypeWarning: Columns (1,2,5,7,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('Eartquakes-1990-2023.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Eartquakes-1990-2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_14544\\3310454464.py:4: DtypeWarning: Columns (1,2,5,7,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('Eartquakes-1990-2023.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to new_file_last_25000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_14544\\3310454464.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset[['year', 'month', 'day', 'hour']] = df_subset['date'].apply(lambda x: pd.Series(extract_date_components(x)))\n",
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_14544\\3310454464.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset[['year', 'month', 'day', 'hour']] = df_subset['date'].apply(lambda x: pd.Series(extract_date_components(x)))\n",
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_14544\\3310454464.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset[['year', 'month', 'day', 'hour']] = df_subset['date'].apply(lambda x: pd.Series(extract_date_components(x)))\n",
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_14544\\3310454464.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset[['year', 'month', 'day', 'hour']] = df_subset['date'].apply(lambda x: pd.Series(extract_date_components(x)))\n",
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_14544\\3310454464.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset.drop(columns=['date'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from file\n",
    "df = pd.read_csv('Eartquakes-1990-2023.csv')\n",
    "\n",
    "# Drop rows with null values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Select the last 25000 rows\n",
    "num_rows_to_select = min(25000, len(df_cleaned))\n",
    "df_last_25000 = df_cleaned.tail(num_rows_to_select)\n",
    "\n",
    "# Function to extract year, month, day, and hour\n",
    "def extract_date_components(date_str):\n",
    "    date_time_obj = pd.to_datetime(date_str)\n",
    "    return date_time_obj.year, date_time_obj.month, date_time_obj.day, date_time_obj.hour\n",
    "\n",
    "# Extracting specific columns\n",
    "df_subset = df_last_25000[['longitude', 'latitude', 'significance', 'magnitudo', 'date']]\n",
    "\n",
    "# Extracting year, month, day, and hour\n",
    "df_subset[['year', 'month', 'day', 'hour']] = df_subset['date'].apply(lambda x: pd.Series(extract_date_components(x)))\n",
    "\n",
    "# Dropping the original 'date' column\n",
    "df_subset.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Save df_subset to a new CSV file\n",
    "df_subset.to_csv('new_file_last_25000.csv', index=False)\n",
    "\n",
    "print(\"Data saved to new_file_last_25000.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-149.6692      61.7302      96.        ...    1.           1.\n",
      "     0.       ]\n",
      " [-155.2123333   19.3176667   31.        ...    1.           1.\n",
      "     0.       ]\n",
      " [-122.8061667   38.821       19.        ...    1.           1.\n",
      "     0.       ]\n",
      " ...\n",
      " [-121.6051667   36.9648333   29.        ...    4.          28.\n",
      "     5.       ]\n",
      " [-122.0171667   37.8551667   27.        ...    4.          28.\n",
      "     5.       ]\n",
      " [-122.0343333   37.8743333   12.        ...    4.          28.\n",
      "     5.       ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20104, 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('new_file_last_25000.csv', delimiter=',',skiprows=1)\n",
    "\n",
    "# # Drop the first row\n",
    "# data_array = data_array[1:]\n",
    "\n",
    "print(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (20104, 6)\n",
      "y shape:  (20104, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extracting features (longitude, latitude, year, month, day, hour)\n",
    "X = data_array[:, [0, 1, 4, 5, 6, 7]]\n",
    "\n",
    "# Extracting target variable (magnitudo)\n",
    "y = data_array[:, [3]]\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (12062, 6)\n",
      "y train shape:  (12062, 1)\n",
      "X CV shape: (4021, 6)\n",
      "X CV shape: (4021, 1)\n",
      "X test shape: (4021, 6)\n",
      "X test shape: (4021, 1)\n"
     ]
    }
   ],
   "source": [
    "## STEP 2\n",
    "\n",
    "x_train, x_, y_train, y_ = train_test_split(X, y, test_size=0.40, random_state=1)\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "del x_, y_\n",
    "\n",
    "print(\"X train shape: \",x_train.shape)\n",
    "print(\"y train shape: \",y_train.shape)\n",
    "\n",
    "print(\"X CV shape:\",x_cv.shape)\n",
    "print(\"X CV shape:\",y_cv.shape)\n",
    "\n",
    "print(\"X test shape:\",x_test.shape)\n",
    "print(\"X test shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 \n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Polynomial feature\n",
    "poly = PolynomialFeatures(degree = 3, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "X_test_mapped = poly.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using the z-score\n",
    "scaler = StandardScaler()\n",
    "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
    "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  0.26294147373786747\n",
      "Cross-Validation error:  0.2736003999139985\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_mapped_scaled,y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "yhat = model.predict(X_train_mapped_scaled)\n",
    "print(\"Training error: \", mean_squared_error(y_train,yhat) / 2)\n",
    "\n",
    "yhat = model.predict(X_cv_mapped_scaled)\n",
    "print(\"Cross-Validation error: \", mean_squared_error(y_cv,yhat) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/C2_W3_poly.png' width=50%>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mses = []\n",
    "cv_mses = []\n",
    "models = []\n",
    "polys = []\n",
    "scalers = []\n",
    "\n",
    "for degree in range(1,7):\n",
    "    \n",
    "    # Add polynomial features to the training set\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_train_mapped = poly.fit_transform(x_train)\n",
    "    polys.append(poly)\n",
    "    \n",
    "    # Scale the training set\n",
    "    scaler_poly = StandardScaler()\n",
    "    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "    scalers.append(scaler_poly)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_mapped_scaled, y_train )\n",
    "    models.append(model)\n",
    "    \n",
    "    # Compute the training MSE\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    train_mses.append(train_mse)\n",
    "    \n",
    "    # Add polynomial features and scale the cross validation set\n",
    "    X_cv_mapped = poly.transform(x_cv)\n",
    "    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "    \n",
    "    # Compute the cross validation MSE\n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    cv_mses.append(cv_mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest CV MSE is found in the model with degree=6\n"
     ]
    }
   ],
   "source": [
    "degree = np.argmin(cv_mses) + 1\n",
    "print(f\"Lowest CV MSE is found in the model with degree={degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.21\n",
      "Cross Validation MSE: 0.24\n",
      "Test MSE: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Add polynomial features to the test set\n",
    "X_test_mapped = polys[degree-1].transform(x_test)\n",
    "\n",
    "# Scale the test set\n",
    "X_test_mapped_scaled = scalers[degree-1].transform(X_test_mapped)\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = models[degree-1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Training MSE: {train_mses[degree-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {cv_mses[degree-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  0.26294147373786747\n",
      "Cross-Validation error:  0.2736003999139985\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_mapped_scaled,y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "yhat = model.predict(X_train_mapped_scaled)\n",
    "print(\"Training error: \", mean_squared_error(y_train,yhat) / 2)\n",
    "\n",
    "yhat = model.predict(X_cv_mapped_scaled)\n",
    "print(\"Cross-Validation error: \", mean_squared_error(y_cv,yhat) / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "The same model selection process can also be used when choosing between different neural network architectures. In this section, you will create the models shown below and apply it to the same regression task above.\n",
    "\n",
    "<img src='images\\C2_W3_NN_Arch.png' width=40%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features\n",
    "degree = 1\n",
    "poly = PolynomialFeatures(degree, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "X_test_mapped = poly.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using the z-score\n",
    "scaler = StandardScaler()\n",
    "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
    "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models():\n",
    "    \n",
    "    tf.random.set_seed(20)\n",
    "    \n",
    "    model_1 = Sequential(\n",
    "        [\n",
    "            Dense(25, activation = 'relu'),\n",
    "            Dense(15, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_1'\n",
    "    )\n",
    "\n",
    "    model_2 = Sequential(\n",
    "        [\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_2'\n",
    "    )\n",
    "\n",
    "    model_3 = Sequential(\n",
    "        [\n",
    "            Dense(32, activation = 'relu'),\n",
    "            Dense(16, activation = 'relu'),\n",
    "            Dense(8, activation = 'relu'),\n",
    "            Dense(4, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_3'\n",
    "    )\n",
    "    \n",
    "    model_list = [model_1, model_2, model_3]\n",
    "    \n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model_1...\n",
      "Done!\n",
      "\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step\n",
      "Training model_2...\n",
      "Done!\n",
      "\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step\n",
      "Training model_3...\n",
      "Done!\n",
      "\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step\n",
      "RESULTS:\n",
      "Model 1: Training MSE: 0.44, CV MSE: 0.46\n",
      "Model 2: Training MSE: 0.35, CV MSE: 0.37\n",
      "Model 3: Training MSE: 0.75, CV MSE: 0.81\n"
     ]
    }
   ],
   "source": [
    "nn_train_mses = []\n",
    "nn_cv_mses = []\n",
    "\n",
    "nn_models = build_models()\n",
    "\n",
    "for model in nn_models:\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_mapped_scaled, y_train,\n",
    "        epochs=100,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "\n",
    "    # Record the training MSEs\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    nn_train_mses.append(train_mse)\n",
    "    \n",
    "    # Record the cross validation MSEs \n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    nn_cv_mses.append(cv_mse)\n",
    "\n",
    "    \n",
    "# print results\n",
    "print(\"RESULTS:\")\n",
    "for model_num in range(len(nn_train_mses)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n",
    "        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step\n",
      "Selected Model: 2\n",
      "Training MSE: 0.35\n",
      "Cross Validation MSE: 0.37\n",
      "Test MSE: 0.36\n"
     ]
    }
   ],
   "source": [
    "# Select the model with the lowest CV MSE\n",
    "model_num = 2\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = nn_models[model_num-1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
